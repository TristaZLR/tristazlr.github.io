{"title":"Data Analysis with Python 2","date":"2019-05-15T12:30:00.000Z","thumbnail":"https://i.postimg.cc/8PWV2Cvv/depositphotos-43981823-stock-photo-word-cloud-data-analysis.jpg","slug":"Data Analysis with Python 2","tags":["Data Science","python"],"categories":["学习 - DA"],"updated":"2019-05-15T13:15:00.795Z","content":"<h1 id=\"Data-Analysis-with-Python-2\"><center>Data Analysis with Python 2</center><a href=\"post/Data Analysis with Python 2#Data-Analysis-with-Python-2\"></a></h1><h2 id=\"Exploratory-Data-Analysis\">Exploratory Data Analysis<a href=\"post/Data Analysis with Python 2#Exploratory-Data-Analysis\"></a></h2><h3 id=\"Descriptive-Statistics\">Descriptive Statistics<a href=\"post/Data Analysis with Python 2#Descriptive-Statistics\"></a></h3><p>在对数据进行分析之前，我们需要对数据建模。一种简单的方法是先对数据进行描述性统计。  </p>\n<ul>\n<li><p>pandas 的 describe()方法它能计算出均值（mean），数据量，标准差，四分位数（the quartiles）和极端值<br><code>df.describe()</code></p>\n</li>\n<li><p>使用value_counts来总结分类变量<br><code>drive_wheels_counts=df[&#39;drive_wheels&#39;].value_counts()</code></p>\n</li>\n<li><p>Boxplots可以可视化数值型数据<br><code>sns.boxplot(x=&quot; &quot;,y=&quot; &quot;,data=df)</code></p>\n</li>\n<li><p>scatter plot能够展示两个变量之间的关系<br>通常情况下需要预测的数据放在y轴  </p>\n  <figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">y=df[&quot;列名&quot;]  </span><br><span class=\"line\">x=df[&quot;列名&quot;]  </span><br><span class=\"line\">plt.scatter(x,y)  </span><br><span class=\"line\">plt.titile(&quot;title name&quot;)  </span><br><span class=\"line\">plt.xlable(&quot;x name&quot;)  </span><br><span class=\"line\">plr.ylable(&quot;y name&quot;)</span><br></pre></td></tr></table></figure>\n</li>\n</ul>\n<h3 id=\"GroupBy-in-Python\">GroupBy in Python<a href=\"post/Data Analysis with Python 2#GroupBy-in-Python\"></a></h3><p>基础分组第一步<br>挑选出需要的列<br><code>df_test = df[[&quot;列名&quot;,&quot;列名&quot;,&quot;列名&quot;]]</code><br>第二步按需求分组<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">df_grp = df_test.groupby([&quot;列名&quot;,&quot;列名&quot;],as_index=False).mean()</span><br><span class=\"line\">df_grp</span><br></pre></td></tr></table></figure></p>\n<p>第三步使用pandas的pivot优化图像，横轴只放变量，其他放竖轴<br><code>df_pivot = df_grp.pivot(index=&#39;xxx&#39;,columns=&#39;xxx&#39;)</code><br>还可以使用heat map<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">plt.pcolor(df_pivot, cmap=&apos;RdBu&apos;)</span><br><span class=\"line\">plt.colorbar()</span><br><span class=\"line\">plt.show()</span><br></pre></td></tr></table></figure></p>\n<h3 id=\"Analysis-of-Variance-ANOVA\">Analysis of Variance(ANOVA)<a href=\"post/Data Analysis with Python 2#Analysis-of-Variance-ANOVA\"></a></h3><p>ANOVA可以查看哪个元素对结果的预测影响更大<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">df_anova=df[[&quot;make&quot;,&quot;price&quot;]]  </span><br><span class=\"line\">grouoped_anova= df_anova.groupby([&quot;make&quot;])  </span><br><span class=\"line\">anava_results_1=status.f_oneway(grouped_anova.get_group(&quot;honda&quot;)  </span><br><span class=\"line\">[&quot;price&quot;],grouped_anova.get_group(&quot;subaru&quot;)[&quot;price&quot;])</span><br></pre></td></tr></table></figure></p>\n<h3 id=\"Correlation\">Correlation<a href=\"post/Data Analysis with Python 2#Correlation\"></a></h3><p>测量两个变量的相关性<br>a scatter plot and an added linear line called a regression line<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">sns.regplot(x=&quot;xxx&quot;,y=&quot;xxx&quot;,data=df)</span><br><span class=\"line\">plt.ylim(0,)</span><br></pre></td></tr></table></figure></p>\n<h3 id=\"Correlation-Statistics\">Correlation Statistics<a href=\"post/Data Analysis with Python 2#Correlation-Statistics\"></a></h3><p>满足两点则证明两者之间有很强的关联性<br>Correlation coefficient close to 1 or -1<br>P value less than 0.001<br><code>pearson_coef, p_value = stats.pearsonr(df[&#39;xxx&#39;] , df[&#39;xxx&#39;])</code></p>\n<h2 id=\"Model-Development\">Model Development<a href=\"post/Data Analysis with Python 2#Model-Development\"></a></h2><p>simple linear regression and multiple linear regression  </p>\n<ul>\n<li><p>SLR: y=b<sub>0</sub>+b<sub>1</sub>x<br>To fit the model in Python, first we import linear model from scikit-learn  </p>\n  <figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">from sklearn.linear_model import LinearRegresstion</span><br><span class=\"line\">lm=LinearRegression</span><br><span class=\"line\">X=df[[&apos;predictor&apos;]]</span><br><span class=\"line\">Y=df[[&apos;target&apos;]]</span><br><span class=\"line\">lm.fit(X,Y)</span><br><span class=\"line\">Yhat=lm.predict(X)</span><br><span class=\"line\">lm.intercept</span><br><span class=\"line\">lm.coef //查看截距和系数</span><br></pre></td></tr></table></figure>\n</li>\n<li><p>MLR：Y=b<sub>0</sub>+b<sub>1</sub>x<sub>1</sub>+b<sub>2</sub>x<sub>2</sub>+b<sub>3</sub>x<sub>3</sub>+…+b<sub>n</sub>x<sub>n</sub>  </p>\n  <figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">z=df[[&apos;A&apos;],[&apos;B&apos;],[&apos;C&apos;],[&apos;D&apos;]]</span><br><span class=\"line\">lm.fit(z,df[&apos;E&apos;])</span><br><span class=\"line\">Yhat=lm.predict(X)</span><br><span class=\"line\">lm.intercept</span><br><span class=\"line\">lm.coef</span><br></pre></td></tr></table></figure>\n</li>\n</ul>\n<h3 id=\"model-evaluation-using-visualization\">model evaluation using visualization<a href=\"post/Data Analysis with Python 2#model-evaluation-using-visualization\"></a></h3><p>Regression Plot<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">import seaborn as sns</span><br><span class=\"line\">sns.plot(x=&quot;a&quot;,y=&quot;b&quot;,data=df)</span><br><span class=\"line\">plt.ylim(0,)</span><br></pre></td></tr></table></figure></p>\n<p>The residual plot represents the error between the actual values<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">sns.residplot(df[&apos;feature&apos;],df[‘feature’])</span><br></pre></td></tr></table></figure></p>\n<p>一般图像的点分散分布，没有规律，图像没有曲率，则模型正确<br>A distribution plot counts the predicted value versus the actual value<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">axl=sns.displot(df[&apos;target&apos;],hist=False,color=&quot;r&quot;,label=&quot;Actual Value&quot;)</span><br><span class=\"line\">sns.distplot(Yhat,hist=False,color=&quot;b&quot;,label=&quot;Fitted Values&quot;,ax=axl)</span><br></pre></td></tr></table></figure></p>\n<h3 id=\"Polynomial-Regression-and-Pipelines\">Polynomial Regression and Pipelines<a href=\"post/Data Analysis with Python 2#Polynomial-Regression-and-Pipelines\"></a></h3><ul>\n<li><p>计算三次方程时  </p>\n  <figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">f=np.polyfit(x,y,3)</span><br><span class=\"line\">p=np.polyld(f)</span><br><span class=\"line\">print(p)</span><br></pre></td></tr></table></figure>\n</li>\n<li><p>计算多维多项式<br>numpy的polyfit方法可以处理多阶函数，但类似于下面过于复杂的函数polyfit无法处理<br>于是，需要用scikit-learn的“processing”library来创建一个多项特征对象<br>use the “preprocessing” library in scikit-learn</p>\n  <figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">from sklearn.preprocessing import PolynomialFeatures</span><br><span class=\"line\">pr=polynomialFeatures(degree=2,include_bias=False)//degree作为参数</span><br><span class=\"line\">x_polly=pr.fit_transform(x[&apos;A&apos;,&apos;B&apos;])//将特征值转换到多项式特征值中</span><br></pre></td></tr></table></figure>\n</li>\n</ul>\n<p>实际解决问题时遇到的数据肯定不值这点，所以我们需要用scikit-learn来规范特征<br>支持同时规范每个特征<br>Standardize each feature simultaneously<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">from sklearn.preprocessing import StandardScaler//引入包</span><br><span class=\"line\">SCALE=StandardScaler()//实例化</span><br><span class=\"line\">SCALE.fit(x_data[[&apos;A&apos;,&apos;B&apos;]])//规范化数值</span><br><span class=\"line\">x_scale=SCALE.transform(x_data[[&apos;A&apos;,&apos;B&apos;]])//存到新矩阵</span><br><span class=\"line\">```  </span><br><span class=\"line\">Pipelines可以简化以上方法的代码    </span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">1. 第一步引入需要的包</span><br></pre></td></tr></table></figure></p>\n<pre><code>from sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.pipeline import Pipeline\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"></span><br><span class=\"line\">2. 第二步建立元组列表</span><br></pre></td></tr></table></figure>\n\nInput=[(&apos;Scale&apos;,StandardScaler()),(&apos;polynomial&apos;,PolynomialFeatures(degree=2)),(&apos;model&apos;,LinearRegression)]\npipe=Pipeline(Input)\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"></span><br><span class=\"line\">3. 第三步训练pipeline</span><br></pre></td></tr></table></figure>\n\npipe.train(X(&apos;A&apos;,&apos;B&apos;,&apos;C&apos;,&apos;D&apos;),y)\nyhat=Pipe.predict(X[[&apos;A&apos;,&apos;B&apos;,&apos;C&apos;,&apos;D&apos;]])\n```\n该方法对数据进行归一化，执行多项式变换，然后输出预测。\n</code></pre><h3 id=\"Measures-for-In-Sample-Evaluation\">Measures for In-Sample Evaluation<a href=\"post/Data Analysis with Python 2#Measures-for-In-Sample-Evaluation\"></a></h3><p>前面几种方法都是视觉上来评判模型的好坏，还有一种方法是数值上来判断模型的好坏  </p>\n<ul>\n<li>Mean Squared Error  </li>\n<li>R Squared  </li>\n</ul>\n<h3 id=\"Prediction-and-Decision-Making\">Prediction and Decision Making<a href=\"post/Data Analysis with Python 2#Prediction-and-Decision-Making\"></a></h3><p>在决策出最合适的模型时，需要考虑到你的预测值的合理性<br>图像，数值多方面比较后，再选择出最合适的模型</p>\n","next":{"title":"Data Analysis with Python 1","slug":"Data Analysis with Python 1"},"link":"https://www.tristazlr.top/post/Data Analysis with Python 2/","toc":[{"title":"<center>Data Analysis with Python 2</center>","id":"Data-Analysis-with-Python-2","index":"1","children":[{"title":"Exploratory Data Analysis","id":"Exploratory-Data-Analysis","index":"1.1","children":[{"title":"Descriptive Statistics","id":"Descriptive-Statistics","index":"1.1.1"},{"title":"GroupBy in Python","id":"GroupBy-in-Python","index":"1.1.2"},{"title":"Analysis of Variance(ANOVA)","id":"Analysis-of-Variance-ANOVA","index":"1.1.3"},{"title":"Correlation","id":"Correlation","index":"1.1.4"},{"title":"Correlation Statistics","id":"Correlation-Statistics","index":"1.1.5"}]},{"title":"Model Development","id":"Model-Development","index":"1.2","children":[{"title":"model evaluation using visualization","id":"model-evaluation-using-visualization","index":"1.2.1"},{"title":"Polynomial Regression and Pipelines","id":"Polynomial-Regression-and-Pipelines","index":"1.2.2"},{"title":"Measures for In-Sample Evaluation","id":"Measures-for-In-Sample-Evaluation","index":"1.2.3"},{"title":"Prediction and Decision Making","id":"Prediction-and-Decision-Making","index":"1.2.4"}]}]}],"copyright":{"author":"Trista's Blog","link":"<a href=\"https://www.tristazlr.top/post/Data Analysis with Python 2/\" title=\"Data Analysis with Python 2\">https://www.tristazlr.top/post/Data Analysis with Python 2/</a>","license":"Attribution-NonCommercial-NoDerivatives 4.0 International (<a href=\"https://creativecommons.org/licenses/by-nc-sa/4.0/\" rel=\"external nofollow noopener\" target=\"_blank\">CC BY-NC-ND 4.0</a>)"}}